{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90714a6f",
   "metadata": {},
   "source": [
    "Sin tf.data Fem BL vs W7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ccaf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/PC-EIAD209/Desktop/AnaKei/NIPD-AI\") # Add the project root to the sys.path\n",
    "\n",
    "#sys.path.append(\"C:\\\\Users\\\\\"+os.getlogin()+\"\\\\OneDrive - Instituto Tecnologico y de Estudios Superiores de Monterrey\\\\PainClassifier\")\n",
    "from my_data_generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bdcc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import scipy.io as sio\n",
    "import os\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support as score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "import scipy as sp\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Conv3D, MaxPooling3D, Flatten, Dropout, GlobalAveragePooling3D, concatenate, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from keras.regularizers import l2\n",
    "import cv2\n",
    "from keras import initializers\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "import tensorflow as tf\n",
    "from keras import losses\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback\n",
    "import gc\n",
    "#from numba import cuda\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import nibabel as nib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d3b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference volume for overlaying heatmaps\n",
    "rabies_ref_path= r\"F:\\New data\\sigma_files\\sigma_files\\SIGMA_resam_InVivo_Brain_Template_Masked.nii\"\n",
    "rabies_vol = nib.load(rabies_ref_path).get_fdata()\n",
    "#rabies_vol= np.mean(rabies_ref, axis=3)\n",
    "print(\"rabies_vol shape:\",rabies_vol.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d9153",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Devices:\", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514be167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fucntion to filter heatmap (only keep high activations)\n",
    "\n",
    "def filter_heatmap(heatmap, threshold=0.6):\n",
    "  \n",
    "    # copy to avoid modifying original\n",
    "    filtered = np.copy(heatmap)\n",
    "\n",
    "    # range [0, 1]\n",
    "    if np.max(filtered) > 1:\n",
    "        filtered = filtered / 255.0\n",
    "\n",
    "    # Apply adaptive threshold\n",
    "    dynamic_threshold = threshold * np.max(filtered)\n",
    "    filtered[filtered < dynamic_threshold] = 0\n",
    "\n",
    "    # normalize again to 0–255\n",
    "    filtered = 255 * filtered / np.max(filtered) if np.max(filtered) > 0 else filtered\n",
    "\n",
    "    filtered = np.uint8(filtered)\n",
    "\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e14d56",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e88e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG16_3D(blocks):\n",
    "        \n",
    "    inputs = Input(shape=(42, 65, 29), name='input_layer')\n",
    "    x = Reshape(target_shape=[42, 65, 29, 1], name='input_x_3d_volumes')(inputs)\n",
    "\n",
    "    if blocks == 1:\n",
    "        print(\"entra al 1\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        \n",
    "    elif blocks == 2:\n",
    "        print(\"entra al 2\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "            \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 2nd Conv Block\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        \n",
    "    elif blocks == 3:\n",
    "        print(\"entra al 3\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "            \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 2nd Conv Block\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 3rd Conv block  \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        \n",
    "    elif blocks == 4:\n",
    "        print(\"entra al 4\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "            \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 2nd Conv Block\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 3rd Conv block  \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        #x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 4th Conv block\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        #x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    elif blocks == 5:\n",
    "        print(\"entra al 5\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "            \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 2nd Conv Block\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =1, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 3rd Conv block  \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x) \n",
    "        #x = MaxPooling3D(pool_size =2, strides =1, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 4th Conv block\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =1, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 5th Conv block\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Fully connected layers  \n",
    "    x = Flatten()(x) \n",
    "    x = Dense(units = 4096, activation ='relu',kernel_regularizer='l2')(x)\n",
    "    #x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = Dense(units = 4096, activation ='relu',kernel_regularizer='l2')(x) \n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    output = Dense(units = 2,activation ='sigmoid',kernel_regularizer='l2')(x)\n",
    "    \n",
    "    # creating the model\n",
    "    VGG_3d_model = Model (inputs=inputs, outputs =output)\n",
    "    #model.summary()\n",
    "\n",
    "    return VGG_3d_model\n",
    "\n",
    "def set_pretrained_weigths(VGG_3d_model):\n",
    "    #VGG 16 with weights from Imagenet\n",
    "    pretrained_model = tf.keras.applications.VGG16(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        pooling='avg',\n",
    "        input_shape = (42, 65, 3)\n",
    "    )\n",
    "    \n",
    "    #conv layers on VGG_3d_model\n",
    "    layers_conv = []\n",
    "    for j in range(len(VGG_3d_model.layers)):\n",
    "        if \"conv3d\" in str(VGG_3d_model.layers[j]):\n",
    "            layers_conv.append(j)\n",
    "    layers_conv_pretrained = []\n",
    "    for j in range(len(pretrained_model.layers)):\n",
    "        if \"Conv2D\" in str(pretrained_model.layers[j]):\n",
    "            layers_conv_pretrained.append(j)\n",
    "    \n",
    "    for i in range(len(layers_conv)):\n",
    "        if \"Conv2D\" in str(pretrained_model.layers[layers_conv_pretrained[i]]):\n",
    "            if i == 0:\n",
    "                w = pretrained_model.layers[layers_conv_pretrained[i]].get_weights()[0].sum(axis=2, keepdims=True)\n",
    "            else:\n",
    "                w = pretrained_model.layers[layers_conv_pretrained[i]].get_weights()[0]\n",
    "                \n",
    "            w3d=[]\n",
    "            \n",
    "            w = np.reshape(w,(3,3,-1),order='F')\n",
    "            for j in range(len(w[0,0,:])):\n",
    "                for k in range(3):\n",
    "                    w3d.append(w[:,:,j])\n",
    "            w3d = np.transpose(w3d, (1,2,0))\n",
    "            \n",
    "            new_weights = np.reshape(w3d, np.array(VGG_3d_model.layers[layers_conv[i]].get_weights()[0]).shape,order='F')\n",
    "            new_bias = pretrained_model.layers[layers_conv_pretrained[i]].get_weights()[1]\n",
    "            \n",
    "            WnB = []\n",
    "            WnB.append(new_weights)\n",
    "            WnB.append(new_bias)\n",
    "    \n",
    "            VGG_3d_model.layers[layers_conv[i]].set_weights(WnB)\n",
    "\n",
    "    del pretrained_model, w, WnB, new_weights, new_bias, w3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd283a40",
   "metadata": {},
   "source": [
    "METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d63695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusionmatrix_multiclass(y_test,pred):\n",
    "    cm = confusion_matrix(y_test, (np.rint(preds)).astype(int) )\n",
    "    group_names = ['True baseline','False Baseline','False Baseline',   \n",
    "                   'False week 1','Truec','False Week 1',\n",
    "                  'False week 7','False week 7','True week 7']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         np.ndarray.flatten(cm/(np.sum(cm,axis=1).reshape(3,1)))]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(3,3)\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', xticklabels = ['Baseline','Week 1','Week 7'] ,yticklabels = ['Baseline','Week 1','Week 7'])\n",
    "    plt.show()\n",
    "\n",
    "def confusionmatrix(y_test,preds):\n",
    "    #Construct the Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    group_names = ['True Naive','False Naive','False CPH','True CPH']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         np.ndarray.flatten(cm/(np.sum(cm,axis=1).reshape(2,1)))]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', xticklabels = ['BL','W7'] ,yticklabels = ['BL','W7'])\n",
    "    plt.show()\n",
    "    return sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', xticklabels = ['BL','W7'] ,yticklabels = ['BL','W7'])\n",
    "    \n",
    "def confusionmatrix_binary(y_test, preds):\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    group_names = ['True baseline','False baseline','False Week 1','True Week 1']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         np.ndarray.flatten(cm/(np.sum(cm,axis=1).reshape(2,1)))]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', xticklabels = ['BL','W7'] ,yticklabels = ['BL','W7'])\n",
    "    plt.show()\n",
    "\n",
    "def ROC(probs,y_test): #binary\n",
    "    #Classification Area under curve\n",
    "     warnings.filterwarnings('ignore')\n",
    "             \n",
    "     auc = roc_auc_score(y_test, probs)\n",
    "     print('AUC - Test Set: %.2f%%' % (auc*100))\n",
    "    \n",
    "     # calculate roc curve\n",
    "     fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "     # plot no skill\n",
    "     plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "     # plot the roc curve for the model\n",
    "     plt.plot(fpr, tpr, marker='.')\n",
    "     plt.xlabel('False positive rate')\n",
    "     plt.ylabel('Sensitivity/ Recall')\n",
    "     # show the plot\n",
    "     plt.show()\n",
    "    \n",
    "     probs = (np.rint(probs)).astype(int)   \n",
    "        \n",
    "     precision = precision_score(y_test, probs)\n",
    "     print('Precision: %f' % precision)\n",
    "     # recall: tp / (tp + fn)\n",
    "     recall = recall_score(y_test, probs)\n",
    "     print('Recall: %f' % recall)\n",
    "     # f1: tp / (tp + fp + fn)\n",
    "     f1 = f1_score(y_test, probs)\n",
    "     print('F1 score: %f' % f1)\n",
    "        \n",
    "def ROC_multiclass(model, y_test, n_class):\n",
    "    #y_test: array size (# of subjects, ) with classes \n",
    "    #pretrained model to be evaluated \n",
    "    \n",
    "    label_binarizer = LabelBinarizer().fit(y_test)\n",
    "    y_onehot_test = label_binarizer.transform(y_test)\n",
    "    y_onehot_test.shape  # (n_samples, n_classes)\n",
    "\n",
    "    y_score = model.predict(X_test) # y_score is onehot\n",
    "    \n",
    "    # store the fpr, tpr, and roc_auc for all averaging strategies\n",
    "    fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_onehot_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    print(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['micro']:.2f}\")\n",
    "    \n",
    "    n_classes = n_class\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_onehot_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    fpr_grid = np.linspace(0.0, 1.0, 1000)\n",
    "\n",
    "    # Interpolate all ROC curves at these points\n",
    "    mean_tpr = np.zeros_like(fpr_grid)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])  # linear interpolation\n",
    "\n",
    "    # Average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = fpr_grid\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    print(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")\n",
    "    \n",
    "    target_names = ['Naive','Week1','Week7']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    plt.plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=f\"micro-average ROC curve (AUC = {roc_auc['micro']:.2f})\",\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=f\"macro-average ROC curve (AUC = {roc_auc['macro']:.2f})\",\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "    for class_id, color in zip(range(n_classes), colors):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_onehot_test[:, class_id],\n",
    "            y_score[:, class_id],\n",
    "            name=f\"ROC curve for {target_names[class_id]}\",\n",
    "            color=color,\n",
    "            ax=ax,\n",
    "            plot_chance_level=(class_id == 2),\n",
    "        )\n",
    "\n",
    "    _ = ax.set(\n",
    "        xlabel=\"False Positive Rate\",\n",
    "        ylabel=\"True Positive Rate\",\n",
    "        title=\"Extension of Receiver Operating Characteristic\\nto One-vs-Rest multiclass\",\n",
    "    )\n",
    "    \n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(histories):\n",
    "    c = ['b','g','r','c','m','y','k','w']\n",
    "    ltr = ['fold 1(train)','fold 2(train)','fold 3(train)','fold 4(train)','fold 5(train)']\n",
    "    lts = ['fold 1(val)','fold 2(val)','fold 3(val)','fold 4(val)','fold 5(val)']\n",
    "    for i in range(len(histories)):\n",
    "        # plot loss\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.title('Cross Entropy Loss')\n",
    "        plt.plot(histories[i].history['loss'], color=c[i], label=ltr[i], linestyle=\"-\")\n",
    "        plt.plot(histories[i].history['val_loss'], color=c[i], label=lts[i], linestyle=\"--\")\n",
    "        # plot accuracy\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.title('Classification Accuracy')\n",
    "        plt.plot(histories[i].history['Accuracy'], color=c[i], label=ltr[i], linestyle=\"-\")\n",
    "        plt.plot(histories[i].history['val_Accuracy'], color=c[i], label=lts[i], linestyle=\"--\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# summarize model performance\n",
    "def summarize_performance(scores):\n",
    "    # print summary\n",
    "    print('Accuracy: mean=%.3f std=%.3f, n=%d' % (np.mean(scores)*100, np.std(scores)*100, len(scores)))\n",
    "    # box and whisker plots of results\n",
    "    plt.boxplot(scores)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03290b68",
   "metadata": {},
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f55115",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "female = [49,50,51,52,65,66,77,78,79,80,81,82,83]\n",
    "\n",
    "y_female = np.ones(len(female))\n",
    "\n",
    "subjects = np.array(female)\n",
    "labels = np.array(list(y_female))\n",
    "sessions = [1,3]\n",
    "MRI_type = \"func\"\n",
    "functional_type = \"rest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38acb7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bb423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "print(animation.writers.list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbae4ecf",
   "metadata": {},
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5575e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "USE_BOOTSTRAP = False  \n",
    "N_BOOTSTRAPS  = 1       \n",
    "START_BOOT_AT = 1         # activates bootstrap in run 2 \n",
    "\n",
    "for boot in range(N_BOOTSTRAPS):\n",
    "    do_bootstrap = (USE_BOOTSTRAP and boot >= START_BOOT_AT)\n",
    "\n",
    "    print(f\"\\n=== Run {boot+1}/{N_BOOTSTRAPS} | bootstrap={do_bootstrap} ===\")\n",
    "\n",
    "    if do_bootstrap:\n",
    "        boot_subjects, boot_labels = resample(\n",
    "            subjects, labels, replace=True, random_state=42 + boot\n",
    "        )\n",
    "    else:\n",
    "        boot_subjects, boot_labels = subjects, labels\n",
    "\n",
    "    sub_trainval, sub_test, y_trainval, y_test = train_test_split(\n",
    "        boot_subjects, boot_labels, test_size=0.2, random_state=42, stratify=boot_labels\n",
    "    )\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=1)\n",
    "\n",
    "    scores, histories = list(), list()\n",
    "    run = 1\n",
    "    for train_ix, val_ix in kfold.split(sub_trainval, y_trainval):\n",
    "        print(\"Run #\", run)\n",
    "\n",
    "        sub_train, sub_val = sub_trainval[train_ix], sub_trainval[val_ix]\n",
    "        y_train,  y_val   = y_trainval[train_ix],  y_trainval[val_ix]\n",
    "\n",
    "        # Set default values\n",
    "        config_defaults = {\n",
    "            \"batch\": 4,\n",
    "        }\n",
    "        \n",
    "        # Initialize wandb with a sample project name\n",
    "        wandb.init(\n",
    "            project=\"FEMALE_Naive_vs_CPH(BLvsW7)\",\n",
    "            notes=\"No pooling. New data augmentation. Z-scoring per batch. Batch = 4. 10 epoch. Just z-scoring and applying RandomFlip. All layers gradcam. lr = 1e-5\",\n",
    "            config=config_defaults\n",
    "        )\n",
    "\n",
    "        # Specify the other hyperparameters to the configuration.\n",
    "        wandb.config.epochs = 10  #50 o 100\n",
    "        wandb.config.sub_batch = 8\n",
    "        wandb.config.sub_batch_ts = 8\n",
    "        wandb.config.subjects = subjects\n",
    "        wandb.config.architecture_name = \"VGG16_3D\"\n",
    "        wandb.config.dataset_name = \"NAIVE vs CPH(CPH[Bl-W1])\"\n",
    "        wandb.config.CNN_blocks = 5\n",
    "        wandb.config.sessions = sessions\n",
    "        wandb.config.vols_per_session_tr = 120\n",
    "        wandb.config.vols_per_session_ts = 120\n",
    "        wandb.config.initial_learning_rate = 1e-5\n",
    "        #wandb.config.lr_decay_rate = 0.95\n",
    "        wandb.config.optimizer = \"Adam\"\n",
    "        \n",
    "\n",
    "        # Data loading with the old loader (FILES_and_LABELS + CustomDataGen)\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        CPHclassTrain = FILES_and_LABELS(sub_train, sessions, MRI_type, functional_type)\n",
    "        CPHclassTest  = FILES_and_LABELS(sub_test,  sessions, MRI_type, functional_type)\n",
    "        CPHclassval   = FILES_and_LABELS(sub_val,   sessions, MRI_type, functional_type)\n",
    "\n",
    "        X_train = CPHclassTrain.get_mask_and_bold()\n",
    "        X_test  = CPHclassTest.get_mask_and_bold()\n",
    "        X_val   = CPHclassval.get_mask_and_bold()\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        print(f\"[OLD] Carga inicial get_mask_and_bold: {t1 - t0:.2f} s\")\n",
    "        print(\"# sesiones Train\", len(X_train))\n",
    "        print(\"# sesiones Test\",  len(X_test))\n",
    "        print(\"# sesiones Val\",   len(X_val))\n",
    "\n",
    "\n",
    "        wandb.config.batch = 4\n",
    "\n",
    "        print(\"sub train:\")\n",
    "        print(np.array(X_train)[:, 0])\n",
    "        print(\"sub test:\")\n",
    "        print(np.array(X_test)[:, 0])\n",
    "        print(\"sub val:\")\n",
    "        print(np.array(X_val)[:, 0])\n",
    "\n",
    "        print(\"# sesiones Train\", len(X_train))\n",
    "        print(\"# sesiones Test\",  len(X_test))\n",
    "        print(\"# sesiones Val\",   len(X_val))\n",
    "        \n",
    "        # === Generadores antiguos (CustomDataGen) ===\n",
    "        traingen = CustomDataGen(\n",
    "            X_train,\n",
    "            batch_size=wandb.config.batch,\n",
    "            subbatch_size=wandb.config.sub_batch,\n",
    "            format=\"just_brain\",\n",
    "            vols=wandb.config.vols_per_session_tr,\n",
    "            num_class=2,\n",
    "            classes=\"CPHvsNAIVEfemale\",\n",
    "            augmentation=True\n",
    "        )\n",
    "        traingen.on_epoch_end()\n",
    "\n",
    "     \n",
    "        testgen  = CustomDataGen(\n",
    "            X_test,\n",
    "            batch_size=1,\n",
    "            subbatch_size=wandb.config.sub_batch_ts,\n",
    "            format=\"just_brain\",\n",
    "            vols=wandb.config.vols_per_session_ts,\n",
    "            num_class=2,\n",
    "            classes=\"CPHvsNAIVEfemale\",\n",
    "            shuffle=False\n",
    "        )\n",
    "        valgen   = CustomDataGen(\n",
    "            X_val,\n",
    "            batch_size=len(X_val),\n",
    "            subbatch_size=30,\n",
    "            format=\"just_brain\",\n",
    "            vols=570,\n",
    "            num_class=2,\n",
    "            classes=\"CPHvsNAIVEfemale\"\n",
    "        )\n",
    "\n",
    "        \n",
    "        # benchmark  (CustomDataGen)\n",
    "  \n",
    "        def benchmark_gen(gen, n_steps=20, label=\"train-old\"):\n",
    "            \n",
    "            n_total = len(gen)\n",
    "            if n_total == 0:\n",
    "                print(f\"[bench/{label}] generador vacío\")\n",
    "                return None\n",
    "\n",
    "            n = min(n_steps, n_total)\n",
    "            t0 = time.perf_counter()\n",
    "            got = 0\n",
    "            for i in range(n):\n",
    "                _x, _y = gen[i]\n",
    "                got += 1\n",
    "            dt = time.perf_counter() - t0\n",
    "\n",
    "            if got == 0:\n",
    "                print(f\"[bench/{label}] no se pudieron leer batches\")\n",
    "                return None\n",
    "\n",
    "            spb = dt / got  \n",
    "            print(f\"[bench/{label}] {got} batches en {dt:.2f}s  ⇒  {spb:.3f} s/batch\")\n",
    "            return spb\n",
    "\n",
    "        print(\"\\n=== Benchmark loader antiguo (CustomDataGen) ===\")\n",
    "        spb_train_old = benchmark_gen(traingen, n_steps=20, label=\"train-old\")\n",
    "        spb_val_old   = benchmark_gen(valgen,   n_steps=20, label=\"val-old\")\n",
    "\n",
    "        wandb.log({\n",
    "            \"old_loader_spb_train\": float(spb_train_old) if spb_train_old is not None else None,\n",
    "            \"old_loader_spb_val\":   float(spb_val_old)   if spb_val_old   is not None else None,\n",
    "        })\n",
    "\n",
    "      \n",
    "        #getting model 3D CNN\n",
    "        print(\"Starting VGG 16 3D-----------------------------------------------------\")\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        CNN = VGG16_3D(3)\n",
    "        set_pretrained_weigths(CNN)\n",
    "\n",
    "        CNN.compile(\n",
    "            loss=tf.nn.softmax_cross_entropy_with_logits,\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=wandb.config.initial_learning_rate),\n",
    "            metrics=[\"Accuracy\"]\n",
    "        )\n",
    "\n",
    "        checkpoint_filepath = os.getcwd() + \"/\" + wandb.run.name\n",
    "\n",
    "        acc_loss_rate = CombineCallback()\n",
    "        \n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            monitor='combine_metric',\n",
    "            mode='max',\n",
    "            save_best_only=True\n",
    "        )\n",
    "        \n",
    "        if os.path.exists(checkpoint_filepath):\n",
    "            print(\"Loading previous weights from:\", checkpoint_filepath)\n",
    "            CNN.load_weights(checkpoint_filepath)\n",
    "        else:\n",
    "            print(\"No previous checkpoint found, starting fresh.\")\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        print(\"Training\")\n",
    "        \n",
    "        history = CNN.fit(\n",
    "            traingen,\n",
    "            epochs=wandb.config.epochs,\n",
    "            validation_data=valgen,\n",
    "            shuffle=True,\n",
    "            callbacks=[\n",
    "                WandbCallback(monitor='combine_metric', mode=\"max\", save_model=False),\n",
    "                acc_loss_rate,\n",
    "                model_checkpoint_callback\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        print('Duration (CNN): {}'.format(end_time - start_time))\n",
    "\n",
    "        print(\"Evaluating best epoch\")\n",
    "        CNN.load_weights(checkpoint_filepath)\n",
    "        CNN.compile(\n",
    "            loss=tf.nn.softmax_cross_entropy_with_logits,\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=wandb.config.initial_learning_rate),\n",
    "            metrics=[\"Accuracy\"]\n",
    "        )\n",
    "        _, acc = CNN.evaluate(testgen, verbose=1)\n",
    "\n",
    "       #Predictions, gradcam\n",
    "        y_test = []\n",
    "        x_vols = []\n",
    "        for i in range(int(len(X_test) * (wandb.config.vols_per_session_ts / wandb.config.sub_batch_ts))):\n",
    "            x, y = testgen[i]\n",
    "            y_test.extend(y)\n",
    "            x_vols.extend(x)\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "        print(\"predicts CNN\")\n",
    "        preds = tf.cast(tf.argmax(CNN.predict(testgen), axis=1), tf.int32)\n",
    "\n",
    "        #Wrong predicted subjects\n",
    "        wrong_labeled_subj = mislabeled_subj(y_test, preds, X_test, wandb.config.vols_per_session_ts)\n",
    "        print(\"mislabeled subjects:\\n\", wrong_labeled_subj)\n",
    "        \n",
    "        scores.append(acc)\n",
    "        histories.append(history)\n",
    "\n",
    "       \n",
    "        if not os.path.exists(os.getcwd()+\"/\"+wandb.run.name+\"/Naive\"): \n",
    "            os.makedirs(os.getcwd()+\"/\"+wandb.run.name+\"/Naive\")\n",
    "        if not os.path.exists(os.getcwd()+\"/\"+wandb.run.name+\"/CPH\"): \n",
    "            os.makedirs(os.getcwd()+\"/\"+wandb.run.name+\"/CPH\")\n",
    "\n",
    "        all_layers = [\n",
    "            layer.name for layer in reversed(CNN.layers)\n",
    "            if len(layer.output_shape) == 5 and (layer.__class__.__name__ == 'ReLU' or isinstance(layer, tf.keras.layers.Conv3D))\n",
    "        ]\n",
    "        \n",
    "        index_naive = index_for_gradcam(0, y_test, preds)\n",
    "        index_cph   = index_for_gradcam(1, y_test, preds)\n",
    "\n",
    "        if index_naive is None:\n",
    "            index_naive = 0\n",
    "            print(\"No se encontró ningún sujeto Naive correctamente clasificado\")\n",
    "            print(\"index_naive se pone en 0 SOLO para poder correr el GradCAM\")\n",
    "\n",
    "        if index_cph is None:\n",
    "            print(\"No se encontró ningún sujeto CPH correctamente clasificado\")\n",
    "            print(\"index_cph se pone en 0 SOLO para poder correr el GradCAM\")\n",
    "            index_cph = 0\n",
    "\n",
    "        rabies_crop = rabies_vol[3:45, 4:69, 7:36]  # original 48,81,48 -> cropped 42,65,29\n",
    "\n",
    "        # GradCAM per class\n",
    "        heatmap_naive = make_gradcam_heatmap(np.expand_dims(x_vols[index_naive], axis=0), CNN, all_layers[0])\n",
    "        heatmap_cph   = make_gradcam_heatmap(np.expand_dims(x_vols[index_cph],   axis=0), CNN, all_layers[0])\n",
    "        \n",
    "        resized_heatmap_naive = get_resized_heatmap(heatmap_naive, rabies_crop.shape[0:3])\n",
    "        resized_heatmap_cph   = get_resized_heatmap(heatmap_cph,   rabies_crop.shape[0:3])\n",
    "\n",
    "        resized_heatmap_naive = (filter_heatmap(resized_heatmap_naive, threshold=0.6))\n",
    "        resized_heatmap_cph   = (filter_heatmap(resized_heatmap_cph,   threshold=0.6))\n",
    "      \n",
    "        # Animation GradCAM\n",
    "        gradcam_naive = create_animation(rabies_crop, 'Naive', heatmap=resized_heatmap_naive)\n",
    "        gradcam_cph   = create_animation(rabies_crop, 'CPH',   heatmap=resized_heatmap_cph)\n",
    "\n",
    "        Writer = animation.writers['ffmpeg']\n",
    "        writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "\n",
    "        name_ani_naive = os.getcwd() + \"/\" + wandb.run.name + \"/Naive/GradCam_Naive(BlVsW1-CPH).mp4\"\n",
    "        gradcam_naive.save(name_ani_naive, writer=writer)\n",
    "\n",
    "        name_ani_CPH = os.getcwd() + \"/\" + wandb.run.name + \"/CPH/GradCam_CPH(BlVsW1-CPH).mp4\"\n",
    "        gradcam_cph.save(name_ani_CPH, writer=writer)\n",
    "\n",
    "        print(\"GradCam All ConvLayers\")\n",
    "        all_layers_gradcam_naive = fuse_layers(all_layers, CNN, [x_vols[index_naive]], 0, emphasize=False)\n",
    "        all_layers_gradcam_cph   = fuse_layers(all_layers, CNN, [x_vols[index_cph]],   0, emphasize=False)\n",
    "\n",
    "        all_layers_animation_naive = create_animation(rabies_crop, 'all_layers_gradcam_Naive', heatmap=all_layers_gradcam_naive)\n",
    "        all_layers_animation_cph   = create_animation(rabies_crop, 'all_layers_gradcam_CPH',   heatmap=all_layers_gradcam_cph)\n",
    "\n",
    "        name_all_naive = os.getcwd() + \"/\" + wandb.run.name + \"/Naive/all_layers_gradcam_Naive.mp4\"\n",
    "        all_layers_animation_naive.save(name_all_naive, writer=writer)\n",
    "\n",
    "        name_all_cph = os.getcwd() + \"/\" + wandb.run.name + \"/CPH/all_layers_gradcam_CPH.mp4\"\n",
    "        all_layers_animation_cph.save(name_all_cph, writer=writer)\n",
    "\n",
    "        print(\"Máximo valor del heatmap CPH:\", np.max(resized_heatmap_cph))\n",
    "        print(\"Máximo valor del heatmap Naive:\", np.max(resized_heatmap_naive))\n",
    "\n",
    "        full_grad_cph   = np.zeros_like(rabies_vol)\n",
    "        full_grad_naive = np.zeros_like(rabies_vol)\n",
    "        full_grad_cph[3:45, 4:69, 7:36]   = resized_heatmap_cph\n",
    "        full_grad_naive[3:45, 4:69, 7:36] = resized_heatmap_naive   \n",
    "\n",
    "        for i in range(rabies_vol.shape[1]):\n",
    "            plt.imshow(cv2.resize(np.rot90(rabies_vol[:, i, :]), dsize=(126, 87)), alpha=0.8, cmap='bone')\n",
    "            plt.imshow(cv2.resize(np.rot90(full_grad_cph[:, i, :]), dsize=(126, 87)), alpha=0.4, cmap='Reds')\n",
    "            plt.axis('off')\n",
    "            plt.savefig(os.getcwd()+\"/\"+wandb.run.name+\"/CPH/\"+str(i)+\".png\")\n",
    "            plt.show()\n",
    "\n",
    "        for i in range(rabies_vol.shape[1]):\n",
    "            plt.imshow(cv2.resize(np.rot90(rabies_vol[:, i, :]), dsize=(126, 87)), alpha=0.8, cmap='bone')\n",
    "            plt.imshow(cv2.resize(np.rot90(full_grad_naive[:, i, :]), dsize=(126, 87)), alpha=0.4, cmap='Reds')\n",
    "            plt.axis('off')\n",
    "            plt.savefig(os.getcwd()+\"/\"+wandb.run.name+\"/Naive/\"+str(i)+\".png\")\n",
    "            plt.show()\n",
    "\n",
    "        np.save(os.getcwd()+\"/\"+wandb.run.name+\"/CPH/Array_GradCam-CPH\",   resized_heatmap_cph)\n",
    "        np.save(os.getcwd()+\"/\"+wandb.run.name+\"/Naive/Array_GradCam-Naive\", resized_heatmap_naive)\n",
    "\n",
    "        print(\"CM CNN\")\n",
    "        cm = confusionmatrix(y_test, preds)\n",
    "\n",
    "        wandb.log({\n",
    "            'test_acc': float(acc),\n",
    "            'time_running': '{}'.format(end_time - start_time),\n",
    "            'confution_matrix': wandb.Image(cm),\n",
    "            'mislabeled_subj': wrong_labeled_subj,\n",
    "            'GradCam_Naive-coronal': wandb.Video(name_ani_naive),\n",
    "            'GradCam-CPH-coronal':   wandb.Video(name_ani_CPH),\n",
    "            'GradCam_Naive_all-layers': wandb.Video(name_all_naive),\n",
    "            'GradCam-CPH_all-layers':   wandb.Video(name_all_cph),\n",
    "            'GradCam-per_frames-Naive': wandb.Image(grad_cam_per_frames(rabies_crop, resized_heatmap_naive, threshold=0.3)),\n",
    "            'GradCam-per_frames-CPH':   wandb.Image(grad_cam_per_frames(rabies_crop, resized_heatmap_cph,   threshold=0.3))\n",
    "        })\n",
    "\n",
    "        run += 1\n",
    "            \n",
    "    print(\"histories and scores from VGG 16 M2D\") \n",
    "    summarize_diagnostics(histories)\n",
    "    summarize_performance(scores)\n",
    "\n",
    "    wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nipd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
