{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a2d530-3ec9-42fd-be5f-e5efd35b0fa7",
   "metadata": {},
   "source": [
    "# Female Naive Vs Female CPH\n",
    "1. Female CPH (baseline)\n",
    "2. Female CPH (Week 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344fc43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Environment and GPU configuration\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Suppress verbose TensorFlow/C++ logs\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"    # 0 = all logs, 3 = only errors\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"  # avoid grabbing all GPU memory\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"   # make Weights & Biases less verbose\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"  #  to fully disable W&B\n",
    "\n",
    "# Limit thread usage (optiona)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"2\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"1\"\n",
    "\n",
    "# Add project root to Python path \n",
    "sys.path.append(r\"C:/Users/PC-EIAD209/Desktop/AnaKei/NIPD-AI\")  #adjust to the actual project folder)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reduce TensorFlow Python-level logging\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "\n",
    "# Ensure GPU memory growth is enabled\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for g in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "    except Exception:\n",
    "        \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ffb1b-7a21-4433-b76f-276108f014de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import  modules\n",
    "\n",
    "from tfdata_generator import*\n",
    "from gradcam_utils import *\n",
    "from  callbacks import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e72bd-7f6a-4a51-9d62-8c9891680af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import scipy.io as sio\n",
    "import os\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support as score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "import scipy as sp\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Conv3D, MaxPooling3D, Flatten, Dropout, GlobalAveragePooling3D, concatenate, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from keras.regularizers import l2\n",
    "import cv2\n",
    "from keras import initializers\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "import tensorflow as tf\n",
    "from keras import losses\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback\n",
    "import gc\n",
    "#from numba import cuda\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import nibabel as nib \n",
    "from tensorflow.data import AUTOTUNE\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, balanced_accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7bf870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference volume for overlaying heatmaps\n",
    "\n",
    "# Option 1: use the SIGMA / template volume \n",
    "\n",
    "# rabies_ref_path = r\"F:/New data/sigma_files/SIGMA_resam_InVivo_Brain_Template_Masked.nii\"\n",
    "# rabies_ref = nib.load(rabies_ref_path).get_fdata()\n",
    "# rabies_vol = np.mean(rabies_ref, axis=3)\n",
    "# print(\"Template rabies_ref shape:\", rabies_ref.shape)\n",
    "# print(\"Template rabies_vol shape (mean over time):\", rabies_vol.shape)\n",
    "\n",
    "# Option 2: use one preprocessed RABIES functional run as background volume.\n",
    "rabies_ref_path = (\n",
    "    r\"F:/rabies/preprocess_batch-001/commonspace_bold/\"\n",
    "    r\"_scan_info_subject_id003.session01_split_name_sub-003_ses-01_desc-o_T2w/_run_None/\"\n",
    "    r\"sub-003_ses-01_task-dist_desc-oa_bold_autobox_combined.nii.gz\"\n",
    ")\n",
    "rabies_ref = nib.load(rabies_ref_path).get_fdata()\n",
    "rabies_vol = np.mean(rabies_ref, axis=3)\n",
    "\n",
    "print(\"rabies_ref shape:\", rabies_ref.shape)\n",
    "print(\"rabies_vol shape (mean over time):\", rabies_vol.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca2d53-8010-4075-a5e1-55782abcec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Devices:\", tf.config.list_physical_devices())\n",
    "tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fucntion to filter heatmap (only keep high activations)\n",
    "\n",
    "def filter_heatmap(heatmap, threshold=0.6):\n",
    "  \n",
    "    # copy to avoid modifying original\n",
    "    filtered = np.copy(heatmap)\n",
    "\n",
    "    # range [0, 1]\n",
    "    if np.max(filtered) > 1:\n",
    "        filtered = filtered / 255.0\n",
    "\n",
    "    # Apply adaptive threshold\n",
    "    dynamic_threshold = threshold * np.max(filtered)\n",
    "    filtered[filtered < dynamic_threshold] = 0\n",
    "\n",
    "    # normalize again to 0–255\n",
    "    filtered = 255 * filtered / np.max(filtered) if np.max(filtered) > 0 else filtered\n",
    "\n",
    "    filtered = np.uint8(filtered)\n",
    "\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25446ae9-81d1-4ee5-a514-bd2439a09094",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00cc49e-1a2f-4bc1-a9d0-f96b762bb778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG16_3D(blocks):\n",
    "        \n",
    "    inputs = Input(shape=(42, 65, 29), name='input_layer')\n",
    "    x = Reshape(target_shape=[42, 65, 29, 1], name='input_x_3d_volumes')(inputs)\n",
    "\n",
    "    if blocks == 1:\n",
    "        print(\"entra al 1\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x) #kernel_regularizer='l2')(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        \n",
    "    elif blocks == 2:\n",
    "        print(\"entra al 2\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "            \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 2nd Conv Block\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        \n",
    "    elif blocks == 3:\n",
    "        print(\"entra al 3\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "            \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 2nd Conv Block\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 3rd Conv block  \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        \n",
    "    elif blocks == 4:\n",
    "        print(\"entra al 4\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "            \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 2nd Conv Block\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 3rd Conv block  \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 4th Conv block\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        #x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    elif blocks == 5:\n",
    "        print(\"entra al 5\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x) #l2=0.05\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "            \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 2nd Conv Block\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x)\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =1, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 3rd Conv block  \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x) \n",
    "        #x = MaxPooling3D(pool_size =2, strides =1, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 4th Conv block\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =1, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 5th Conv block\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Fully connected layers  \n",
    "    #x = Flatten()(x)  \n",
    "    x = Dense(units = 4096, activation ='relu',kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x) \n",
    "    x = Dense(units = 4096, activation ='relu',kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    output = Dense(units = 2,activation ='softmax',kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x)\n",
    "    # creating the model\n",
    "    VGG_3d_model = Model (inputs=inputs, outputs =output)\n",
    "    #model.summary()\n",
    "\n",
    "    return VGG_3d_model\n",
    "\n",
    "def set_pretrained_weigths(VGG_3d_model):\n",
    "    #VGG 16 with weights from Imagenet\n",
    "    pretrained_model = tf.keras.applications.VGG16(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        pooling='avg',\n",
    "        input_shape = (42, 65, 3)\n",
    "    )\n",
    "    \n",
    "    #conv layers on VGG_3d_model\n",
    "    layers_conv = []\n",
    "    for j in range(len(VGG_3d_model.layers)):\n",
    "        if \"conv3d\" in str(VGG_3d_model.layers[j]):\n",
    "            layers_conv.append(j)\n",
    "    layers_conv_pretrained = []\n",
    "    for j in range(len(pretrained_model.layers)):\n",
    "        if \"Conv2D\" in str(pretrained_model.layers[j]):\n",
    "            layers_conv_pretrained.append(j)\n",
    "    \n",
    "    for i in range(len(layers_conv)):\n",
    "        if \"Conv2D\" in str(pretrained_model.layers[layers_conv_pretrained[i]]):\n",
    "            if i == 0:\n",
    "                w = pretrained_model.layers[layers_conv_pretrained[i]].get_weights()[0].sum(axis=2, keepdims=True)\n",
    "            else:\n",
    "                w = pretrained_model.layers[layers_conv_pretrained[i]].get_weights()[0]\n",
    "                \n",
    "            w3d=[]\n",
    "            \n",
    "            w = np.reshape(w,(3,3,-1),order='F')\n",
    "            for j in range(len(w[0,0,:])):\n",
    "                for k in range(3):\n",
    "                    w3d.append(w[:,:,j])\n",
    "            w3d = np.transpose(w3d, (1,2,0))\n",
    "            \n",
    "            new_weights = np.reshape(w3d, np.array(VGG_3d_model.layers[layers_conv[i]].get_weights()[0]).shape,order='F')\n",
    "            new_bias = pretrained_model.layers[layers_conv_pretrained[i]].get_weights()[1]\n",
    "            \n",
    "            WnB = []\n",
    "            WnB.append(new_weights)\n",
    "            WnB.append(new_bias)\n",
    "    \n",
    "            VGG_3d_model.layers[layers_conv[i]].set_weights(WnB)\n",
    "\n",
    "    del pretrained_model, w, WnB, new_weights, new_bias, w3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b73eef-388d-4077-81f8-6eb93c39f318",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732562b4-13a5-447d-8802-6be04933fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusionmatrix_multiclass(y_test,pred):\n",
    "    cm = confusion_matrix(y_test, (np.rint(preds)).astype(int) )\n",
    "    group_names = ['True baseline','False Baseline','False Baseline',   \n",
    "                   'False week 1','Truec','False Week 1',\n",
    "                  'False week 7','False week 7','True week 7']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         np.ndarray.flatten(cm/(np.sum(cm,axis=1).reshape(3,1)))]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(3,3)\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', xticklabels = ['Baseline','Week 1','Week 7'] ,yticklabels = ['Baseline','Week 1','Week 7'])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def confusionmatrix(y_test, preds, xticklabels=('BL','W7'), yticklabels=('BL','W7'), **kwargs):\n",
    "    import numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    y_true = np.asarray(y_test).astype(int).ravel()\n",
    "    y_pred = np.asarray(preds).astype(int).ravel()\n",
    "\n",
    "    # Reenvía cualquier kwarg, p.ej. labels=(0,1)\n",
    "    cm = confusion_matrix(y_true, y_pred, **kwargs)\n",
    "\n",
    "    # Evitar divisiones por cero al calcular porcentajes por fila\n",
    "    row_sum = cm.sum(axis=1, keepdims=True)\n",
    "    safe_row_sum = np.where(row_sum == 0, 1, row_sum)\n",
    "    perc = np.divide(cm, safe_row_sum, where=(safe_row_sum != 0))\n",
    "\n",
    "    labels_txt = [f\"{c}\\n{p:.2%}\" for c, p in zip(cm.flatten(), perc.flatten())]\n",
    "    labels_txt = np.asarray(labels_txt).reshape(cm.shape)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4.5,4))\n",
    "    sns.heatmap(cm, annot=labels_txt, fmt='', cmap='Blues',\n",
    "                xticklabels=xticklabels, yticklabels=yticklabels, ax=ax)\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\"); ax.set_title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    return fig, cm\n",
    "\n",
    "\n",
    "    \n",
    "def confusionmatrix_binary(y_test, preds):\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    group_names = ['True baseline','False baseline','False Week 1','True Week 1']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         np.ndarray.flatten(cm/(np.sum(cm,axis=1).reshape(2,1)))]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', xticklabels = ['BL','W7'] ,yticklabels = ['BL','W7'])\n",
    "    plt.show()\n",
    "\n",
    "def ROC(probs,y_test): #binary\n",
    "    #Classification Area under curve\n",
    "     warnings.filterwarnings('ignore')\n",
    "             \n",
    "     auc = roc_auc_score(y_test, probs)\n",
    "     print('AUC - Test Set: %.2f%%' % (auc*100))\n",
    "    \n",
    "     # calculate roc curve\n",
    "     fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "     # plot no skill\n",
    "     plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "     # plot the roc curve for the model\n",
    "     plt.plot(fpr, tpr, marker='.')\n",
    "     plt.xlabel('False positive rate')\n",
    "     plt.ylabel('Sensitivity/ Recall')\n",
    "     # show the plot\n",
    "     plt.show()\n",
    "    \n",
    "     probs = (np.rint(probs)).astype(int)   \n",
    "        \n",
    "     precision = precision_score(y_test, probs)\n",
    "     print('Precision: %f' % precision)\n",
    "     # recall: tp / (tp + fn)\n",
    "     recall = recall_score(y_test, probs)\n",
    "     print('Recall: %f' % recall)\n",
    "     # f1: tp / (tp + fp + fn)\n",
    "     f1 = f1_score(y_test, probs)\n",
    "     print('F1 score: %f' % f1)\n",
    "        \n",
    "def ROC_multiclass(model, y_test, n_class):\n",
    "    #y_test: array size (# of subjects, ) with classes \n",
    "    #pretrained model to be evaluated \n",
    "    \n",
    "    label_binarizer = LabelBinarizer().fit(y_test)\n",
    "    y_onehot_test = label_binarizer.transform(y_test)\n",
    "    y_onehot_test.shape  # (n_samples, n_classes)\n",
    "\n",
    "    y_score = model.predict(X_test) # y_score is onehot\n",
    "    \n",
    "    # store the fpr, tpr, and roc_auc for all averaging strategies\n",
    "    fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_onehot_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    print(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['micro']:.2f}\")\n",
    "    \n",
    "    n_classes = n_class\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_onehot_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    fpr_grid = np.linspace(0.0, 1.0, 1000)\n",
    "\n",
    "    # Interpolate all ROC curves at these points\n",
    "    mean_tpr = np.zeros_like(fpr_grid)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])  # linear interpolation\n",
    "\n",
    "    # Average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = fpr_grid\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    print(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")\n",
    "    \n",
    "    target_names = ['Naive','Week1','Week7']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    plt.plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=f\"micro-average ROC curve (AUC = {roc_auc['micro']:.2f})\",\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=f\"macro-average ROC curve (AUC = {roc_auc['macro']:.2f})\",\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "    for class_id, color in zip(range(n_classes), colors):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_onehot_test[:, class_id],\n",
    "            y_score[:, class_id],\n",
    "            name=f\"ROC curve for {target_names[class_id]}\",\n",
    "            color=color,\n",
    "            ax=ax,\n",
    "            plot_chance_level=(class_id == 2),\n",
    "        )\n",
    "\n",
    "    _ = ax.set(\n",
    "        xlabel=\"False Positive Rate\",\n",
    "        ylabel=\"True Positive Rate\",\n",
    "        title=\"Extension of Receiver Operating Characteristic\\nto One-vs-Rest multiclass\",\n",
    "    )\n",
    "    \n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(histories):\n",
    "    c = ['b','g','r','c','m','y','k','w']\n",
    "    ltr = ['fold 1(train)','fold 2(train)','fold 3(train)','fold 4(train)','fold 5(train)']\n",
    "    lts = ['fold 1(val)','fold 2(val)','fold 3(val)','fold 4(val)','fold 5(val)']\n",
    "    for i in range(len(histories)):\n",
    "        # plot loss\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.title('Cross Entropy Loss')\n",
    "        plt.plot(histories[i].history['loss'], color=c[i], label=ltr[i], linestyle=\"-\")\n",
    "        plt.plot(histories[i].history['val_loss'], color=c[i], label=lts[i], linestyle=\"--\")\n",
    "        # plot accuracy\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.title('Classification Accuracy')\n",
    "        plt.plot(histories[i].history['accuracy'], color=c[i], label=ltr[i], linestyle=\"-\")\n",
    "        plt.plot(histories[i].history['val_accuracy'], color=c[i], label=lts[i], linestyle=\"--\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# summarize model performance\n",
    "def summarize_performance(scores):\n",
    "    # print summary\n",
    "    print('Accuracy: mean=%.3f std=%.3f, n=%d' % (np.mean(scores)*100, np.std(scores)*100, len(scores)))\n",
    "    # box and whisker plots of results\n",
    "    plt.boxplot(scores)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08627ebe-a7e5-4c2c-a81f-5c3fd1f67025",
   "metadata": {},
   "source": [
    "# Just brain. Female. Naive vs CPH\n",
    "1) Naive (CPH_BL)\n",
    "2) CPH (CPH_W7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31333e60-218e-4b1a-bd56-598d92c0dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "female = [49,50,51,52,65,66,77,78,79,80,81,82,83]\n",
    "\n",
    "y_female = np.ones(len(female))\n",
    "\n",
    "subjects = np.array(female)\n",
    "labels = np.array(list(y_female))\n",
    "sessions = [1,3]\n",
    "MRI_type = \"func\"\n",
    "functional_type = \"rest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c966a-ca55-4ed6-a631-775f18290343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to login to your account first \n",
    "wandb.login()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d868ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics, dataset bencchmark and ETA callbacks:\n",
    "\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Binary AUC that takes the positive column from a 2-class softmax\n",
    "class AUCPos(tf.keras.metrics.Metric):\n",
    "    \n",
    "   # Wrapper around tf.keras.metrics.AUC for binary problems \n",
    "\n",
    "    def __init__(self, name=\"auc\", curve=\"ROC\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self._auc = tf.keras.metrics.AUC(curve=curve, name=name)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # y_true: integer labels 0/1 → float vector (B,)\n",
    "        y_true = tf.cast(tf.reshape(y_true, (-1,)), tf.float32)\n",
    "\n",
    "        # y_pred: if shape is (B, 2), take column 1 (positive class)\n",
    "        if tf.rank(y_pred) > 1:\n",
    "            y_pred = y_pred[..., 1]\n",
    "        y_pred = tf.cast(tf.reshape(y_pred, (-1,)), tf.float32)\n",
    "\n",
    "        return self._auc.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        return self._auc.result()\n",
    "\n",
    "    def reset_states(self):\n",
    "        self._auc.reset_states()\n",
    "\n",
    "\n",
    "class SparseCatAccFixed(tf.keras.metrics.SparseCategoricalAccuracy):\n",
    "    '''\n",
    "    SparseCategoricalAccuracy that always flattens y_true to shape (B,).\n",
    "    This avoids shape mismatches when labels come as (B, 1).\n",
    "    ''' \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.reshape(tf.cast(y_true, tf.int64), (-1,))\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "def benchmark_ds(ds, n_steps=50, label=\"train\"):\n",
    "    '''\n",
    "    it measures the seconds per batch for a tf.data.Dataset, without running\n",
    "    the model. It simply iterates over 'n_steps' batches \n",
    "    '''\n",
    "    try:\n",
    "        it = iter(ds)\n",
    "        # warm-up batch (forces the pipeline to start)\n",
    "        _ = next(it)\n",
    "    except StopIteration:\n",
    "        print(f\"[bench/{label}] empty dataset\")\n",
    "        return None\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    got = 0\n",
    "    for _ in range(n_steps):\n",
    "        try:\n",
    "            xb, yb = next(it)\n",
    "            got += 1\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "    dt = time.perf_counter() - t0\n",
    "    if got == 0:\n",
    "        print(f\"[bench/{label}] could not iterate\")\n",
    "        return None\n",
    "\n",
    "    sec_per_batch = dt / got\n",
    "    print(f\"[bench/{label}] {got} batches in {dt:.2f}s  ⇒  {sec_per_batch:.3f} s/batch\")\n",
    "    return sec_per_batch\n",
    "\n",
    "\n",
    "class BatchTime(tf.keras.callbacks.Callback):\n",
    "    '''\n",
    "    smple callback that prints an approximate ETA for each epoch based\n",
    "    on the observed average time per batch.\n",
    "    '''\n",
    "    def __init__(self, steps_per_epoch):\n",
    "        super().__init__()\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.t0 = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.t0 = time.perf_counter()\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        passed = time.perf_counter() - self.t0\n",
    "        done = batch + 1\n",
    "        sec_per_batch = passed / max(1, done)\n",
    "        remaining = (self.steps_per_epoch - done) * sec_per_batch\n",
    "\n",
    "        # Print only at the beginning and then about 10 times per epoch\n",
    "        if done <= 5 or done % max(1, self.steps_per_epoch // 10) == 0:\n",
    "            print(\n",
    "                f\"[ETA] step {done}/{self.steps_per_epoch} | \"\n",
    "                f\"{sec_per_batch:.2f}s/step | ETA {remaining/3600:.2f}h\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d37c0",
   "metadata": {},
   "source": [
    "Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa879411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USE_BOOTSTRAP = False\n",
    "N_BOOTSTRAPS  = 1\n",
    "START_BOOT_AT = 1  # bootstrap would start at run 2 if enabled\n",
    "\n",
    "# tf.data parallelism (tuned for Windows / external disk)\n",
    "PAR = 2\n",
    "PREFETCH_BUF = PAR\n",
    "\n",
    "# Volumes per session used per epoch (here we keep a fixed value)\n",
    "EPOCH_VOL_SCHEDULE = [120]\n",
    "VOLS_TEST = 120\n",
    "\n",
    "for boot in range(N_BOOTSTRAPS):\n",
    "    do_bootstrap = (USE_BOOTSTRAP and boot >= START_BOOT_AT)\n",
    "    print(f\"\\n=== Run {boot+1}/{N_BOOTSTRAPS} | bootstrap={do_bootstrap} ===\")\n",
    "\n",
    "    if do_bootstrap:\n",
    "        boot_subjects, boot_labels = resample(\n",
    "            subjects, labels, replace=True, random_state=42 + boot\n",
    "        )\n",
    "    else:\n",
    "        boot_subjects, boot_labels = subjects, labels\n",
    "\n",
    "    # Split subject-level data into train+val vs test\n",
    "    sub_trainval, sub_test, y_trainval, y_test = train_test_split(\n",
    "        boot_subjects,\n",
    "        boot_labels,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=boot_labels,\n",
    "    )\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=1)\n",
    "\n",
    "    scores = []\n",
    "    histories = []\n",
    "    run = 1\n",
    "\n",
    "    for train_ix, val_ix in kfold.split(sub_trainval, y_trainval):\n",
    "        print(\"Run #\", run)\n",
    "\n",
    "        sub_train, sub_val = sub_trainval[train_ix], sub_trainval[val_ix]\n",
    "        y_train,  y_val   = y_trainval[train_ix],  y_trainval[val_ix]\n",
    "\n",
    "        # Weights & Biases configuration (tracking hyperparameters and metrics)\n",
    "        config_defaults = {\"batch\": 30}\n",
    "        wandb.init(\n",
    "            project=\"FEMALE_Naive_vs_CPH(BLvsW7)\",\n",
    "            notes=\"tf.data 3D mean; (42,65,29,1); SparseCE probs; flips 3D; AUCPos; ETA visible\",\n",
    "            config=config_defaults,\n",
    "        )\n",
    "        wandb.config.epochs = 10\n",
    "        wandb.config.sub_batch = 8\n",
    "        wandb.config.sub_batch_group = 1\n",
    "        wandb.config.sub_batch_ts = 8\n",
    "        wandb.config.subjects = subjects\n",
    "        wandb.config.architecture_name = \"VGG16_3D\"\n",
    "        wandb.config.dataset_name = \"NAIVE vs CPH (BL vs W7)\"\n",
    "        wandb.config.CNN_blocks = 5\n",
    "        wandb.config.sessions = sessions\n",
    "        wandb.config.vols_per_session_tr = EPOCH_VOL_SCHEDULE[-1]\n",
    "        wandb.config.vols_per_session_ts = VOLS_TEST\n",
    "        wandb.config.initial_learning_rate = 1e-5\n",
    "        wandb.config.optimizer = \"Adam\"\n",
    "\n",
    "        # Cropping coordinates and label task\n",
    "        CROP6 = (3, 4, 7, 45, 69, 36)\n",
    "        TASK_NAME = \"bl_vs_w7\"  # BL=0, W7=1\n",
    "        SEED = 42\n",
    "\n",
    "        SUBBATCH        = int(wandb.config.sub_batch)\n",
    "        SUBBATCH_GROUP  = int(wandb.config.sub_batch_group)\n",
    "        EFFECTIVE_BATCH = SUBBATCH * SUBBATCH_GROUP\n",
    "\n",
    "        # Build file lists for each split (REST functional runs)\n",
    "        CPHclassTrain = FILES_and_LABELS(sub_train, sessions, MRI_type, \"rest\")\n",
    "        CPHclassVal   = FILES_and_LABELS(sub_val,   sessions, MRI_type, \"rest\")\n",
    "        CPHclassTest  = FILES_and_LABELS(sub_test,  sessions, MRI_type, \"rest\")\n",
    "\n",
    "        X_train = CPHclassTrain.get_mask_and_bold()\n",
    "        X_val   = CPHclassVal.get_mask_and_bold()\n",
    "        X_test  = CPHclassTest.get_mask_and_bold()\n",
    "\n",
    "        train_pairs = X_train\n",
    "        val_pairs   = X_val\n",
    "        test_pairs  = X_test\n",
    "\n",
    "        print(\"Train sessions:\", np.array(X_train)[:, 0])\n",
    "        print(\"Test sessions:\",  np.array(X_test)[:, 0])\n",
    "        print(\"Val sessions:\",   np.array(X_val)[:, 0])\n",
    "        print(\"# Train sessions:\", len(X_train))\n",
    "        print(\"# Test sessions:\",  len(X_test))\n",
    "        print(\"# Val sessions:\",   len(X_val))\n",
    "\n",
    "        # Build and compile the 3D VGG16 model\n",
    "        print(\"Starting VGG16_3D -----------------------------------------------------\")\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        CNN = VGG16_3D(5)\n",
    "        print(\"CNN input shape:\", CNN.input_shape)\n",
    "        set_pretrained_weigths(CNN)\n",
    "\n",
    "        CNN.compile(\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=wandb.config.initial_learning_rate\n",
    "            ),\n",
    "            metrics=[\n",
    "                SparseCatAccFixed(name=\"acc\"),\n",
    "                AUCPos(name=\"auc\", curve=\"ROC\"),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        checkpoint_filepath = os.path.join(os.getcwd(), wandb.run.name)\n",
    "\n",
    "        # Callbacks: combined metric, early stopping, model checkpointing\n",
    "        acc_loss_rate = CombineCallback()\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_auc\",\n",
    "            mode=\"max\",\n",
    "            patience=8,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "        )\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            monitor=\"val_auc\",\n",
    "            mode=\"max\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        history = None\n",
    "\n",
    "        # Epoch-by-epoch training loop (rebuilds tf.data datasets every epoch)\n",
    "        for e in range(wandb.config.epochs):\n",
    "            print(f\"\\n=== Epoch {e+1}/{wandb.config.epochs} ===\")\n",
    "\n",
    "            # Volumes per session for this epoch (can be scheduled if needed)\n",
    "            VOLS_PER_SESSION_EPOCH = EPOCH_VOL_SCHEDULE[\n",
    "                min(e, len(EPOCH_VOL_SCHEDULE) - 1)\n",
    "            ]\n",
    "\n",
    "            # Steps per epoch for train and validation\n",
    "            batches_per_session = max(1, VOLS_PER_SESSION_EPOCH // SUBBATCH)\n",
    "            steps_per_epoch     = len(train_pairs) * batches_per_session\n",
    "            validation_steps    = len(val_pairs)   * batches_per_session\n",
    "\n",
    "            print(\n",
    "                f\"[config] SUBBATCH={SUBBATCH} | GROUP={SUBBATCH_GROUP} | \"\n",
    "                f\"EFFECTIVE_BATCH={EFFECTIVE_BATCH} | \"\n",
    "                f\"VOLS_PER_SESSION_EPOCH={VOLS_PER_SESSION_EPOCH}\"\n",
    "            )\n",
    "            print(\n",
    "                \"steps_per_epoch:\",\n",
    "                steps_per_epoch,\n",
    "                \"| validation_steps:\",\n",
    "                validation_steps,\n",
    "            )\n",
    "\n",
    "            # Build epoch-specific datasets with tf.data\n",
    "            train_ds = make_epoch_ds(\n",
    "                train_pairs,\n",
    "                training=True,\n",
    "                epoch=e,\n",
    "                par=PAR,\n",
    "                prefetch_buf=PREFETCH_BUF,\n",
    "                seed=SEED,\n",
    "                subbatch=SUBBATCH,\n",
    "                vols_per_session_epoch=VOLS_PER_SESSION_EPOCH,\n",
    "                crop_idx6=CROP6,\n",
    "                task_name=TASK_NAME,\n",
    "            )\n",
    "            val_ds = make_epoch_ds(\n",
    "                val_pairs,\n",
    "                training=False,\n",
    "                epoch=e,\n",
    "                par=PAR,\n",
    "                prefetch_buf=PREFETCH_BUF,\n",
    "                seed=SEED,\n",
    "                subbatch=SUBBATCH,\n",
    "                vols_per_session_epoch=VOLS_PER_SESSION_EPOCH,\n",
    "                crop_idx6=CROP6,\n",
    "                task_name=TASK_NAME,\n",
    "            )\n",
    "\n",
    "            # Full-loader benchmark only on the first epoch\n",
    "            if e == 0:\n",
    "                t0 = time.perf_counter()\n",
    "                it = iter(train_ds)\n",
    "                n_batches = 0\n",
    "                for _ in range(steps_per_epoch):\n",
    "                    try:\n",
    "                        xb, yb = next(it)\n",
    "                        # Force some computation so the pipeline really executes\n",
    "                        _ = tf.reduce_mean(xb).numpy()\n",
    "                        n_batches += 1\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "                dt = time.perf_counter() - t0\n",
    "                if n_batches > 0:\n",
    "                    print(\n",
    "                        f\"[FULL-LOADER tf.data] {n_batches} batches in {dt:.2f}s \"\n",
    "                        f\"⇒ {dt / n_batches:.3f} s/batch\"\n",
    "                    )\n",
    "\n",
    "            # Optional: tf.data options (non-deterministic for speed)\n",
    "            opts = tf.data.Options()\n",
    "            opts.experimental_deterministic = False\n",
    "            opts.experimental_slack = True\n",
    "            train_ds = train_ds.with_options(opts)\n",
    "            val_ds   = val_ds.with_options(opts)\n",
    "\n",
    "            # Cardinality (may be -1 if unknown)\n",
    "            try:\n",
    "                card_tr = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "            except Exception:\n",
    "                card_tr = -1\n",
    "            try:\n",
    "                card_va = tf.data.experimental.cardinality(val_ds).numpy()\n",
    "            except Exception:\n",
    "                card_va = -1\n",
    "            print(f\"[cardinality] train: {card_tr} | val: {card_va}\")\n",
    "\n",
    "            # Lightweight benchmark of the loader\n",
    "            spb_data = benchmark_ds(\n",
    "                train_ds,\n",
    "                n_steps=min(50, max(10, steps_per_epoch // 20)),\n",
    "                label=\"train\",\n",
    "            )\n",
    "            if spb_data is not None:\n",
    "                print(\n",
    "                    f\"[est_ETA loader] ~{(spb_data * steps_per_epoch) / 3600:.2f} h/epoch\"\n",
    "                )\n",
    "\n",
    "            # Sanity check: one batch from the dataset\n",
    "            xb, yb = next(iter(train_ds))\n",
    "            print(f\"[sanity] X {xb.shape} {xb.dtype} | y {yb.shape} {yb.dtype}\")\n",
    "            # Expected: X (B, 42, 65, 29, 1), y (B,)\n",
    "\n",
    "            # ETA callback (uses steps_per_epoch)\n",
    "            eta_cb = BatchTime(steps_per_epoch)\n",
    "\n",
    "            callbacks = [\n",
    "                WandbCallback(monitor=\"val_auc\", mode=\"max\", save_model=False),\n",
    "                acc_loss_rate,\n",
    "                early_stopping,\n",
    "                model_checkpoint_callback,\n",
    "                eta_cb,\n",
    "            ]\n",
    "\n",
    "            history = CNN.fit(\n",
    "                train_ds,\n",
    "                epochs=e + 1,\n",
    "                initial_epoch=e,\n",
    "                validation_data=val_ds,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                validation_steps=validation_steps,\n",
    "                shuffle=False,\n",
    "                callbacks=callbacks,\n",
    "                verbose=1,\n",
    "            )\n",
    "\n",
    "        # Build full test dataset (all volumes per session)\n",
    "        test_ds = make_full_ds(\n",
    "            test_pairs,\n",
    "            subbatch=int(wandb.config.sub_batch_ts),\n",
    "            crop_idx6=CROP6,\n",
    "            task_name=TASK_NAME,\n",
    "            par=PAR,\n",
    "            prefetch_buf=PREFETCH_BUF,\n",
    "        )\n",
    "\n",
    "        # For consistency, you could also flatten labels here if needed\n",
    "        test_ds = test_ds.map(\n",
    "            lambda x, y: (x, tf.reshape(tf.cast(y, tf.int32), ())),\n",
    "            num_parallel_calls=PAR,\n",
    "        )\n",
    "\n",
    "        test_opts = tf.data.Options()\n",
    "        test_opts.experimental_deterministic = True\n",
    "        test_opts.experimental_slack = True\n",
    "        test_ds = test_ds.with_options(test_opts)\n",
    "\n",
    "        loss, acc, auc_val = CNN.evaluate(test_ds, verbose=1)\n",
    "        print(\n",
    "            \"Test — Loss: {:.4f} | Acc: {:.2f}% | AUC: {:.4f}\".format(\n",
    "                loss, acc * 100, auc_val\n",
    "            )\n",
    "        )\n",
    "\n",
    "        #confusion matrices,\n",
    "        # Grad-CAM visualizations and W&B logging.\n",
    "        \n",
    "       \n",
    "\n",
    "        scores.append(acc)\n",
    "        histories.append(history)\n",
    "        run += 1\n",
    "\n",
    "    print(\"histories and scores from VGG16_3D\")\n",
    "    summarize_diagnostics(histories)\n",
    "    summarize_performance(scores)\n",
    "    wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nipd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
